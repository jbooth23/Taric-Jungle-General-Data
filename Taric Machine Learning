library(e1071) #load the e1071 package to begin applying vector classifier models to Taric jungle data
library(readxl) #this allows us to import our data from excel
library(caTools)
library(ROCR)
library(neuralnet) #loads the neuralnet package to do some attempts at neural networks
library(ISLR2)
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(reticulate)

#in this r file we are going to try to create improved taric jungle models using support vector models (SVMs) and neural networks

setwd("/Users/User/Desktop/League of Legends Excel Databases")
getwd()
taricJg_data = read_excel("TaricGames.xlsx")
taricJg.df = taricJg_data

taricJg.df$WinLoss = factor(taricJg.df$WinLoss, levels = c(0, 1))
svmfit = svm(WinLoss ~ Kills + Deaths + Assists + GameTime + ItemsCompleted + Flash + Ghost + Vision + Sunderer + Conqueror +
               Kayn + NoMythic + Glacial + PTA + Baron + DragonDff + KillDff + Ranged + TowerAdv + CSPerMinute +
               Shurelyias, data = taricJg.df, type = "C-classification", kernel = "linear",
                cost = 0.1, scale = FALSE)
?plot.svm
plot(svmfit, taricJg.df, GameTime ~ KillDff)
svmfit$index
summary(svmfit)

svmfit2 = svm(WinLoss ~ Kills + Deaths + Assists + GameTime + ItemsCompleted + Flash + Ghost + Vision + Sunderer + Conqueror +
               Kayn + NoMythic + Glacial + PTA + Baron + DragonDff + KillDff + Ranged + TowerAdv + CSPerMinute +
               Shurelyias, data = taricJg.df, type = "C-classification", kernel = "linear",
             cost = 10, scale = FALSE)
plot(svmfit2, taricJg.df, GameTime ~ KillDff)
svmfit2$index

set.seed (1)
tune.out = tune (svm , WinLoss ~., data = taricJg.df , kernel = "linear",
                    ranges = list (cost = c (0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
bestmod <- tune.out$best.model #selects the best model
summary (bestmod) #the best model (lowest classification error) is one with c = 0.01

svmfit3 = svm(WinLoss ~ Kills + Deaths + Assists + GameTime + ItemsCompleted + Flash + Ghost + Vision + Sunderer + Conqueror +
                Kayn + NoMythic + Glacial + PTA + Baron + DragonDff + KillDff + Ranged + TowerAdv + CSPerMinute +
                Shurelyias, data = taricJg.df, type = "C-classification", kernel = "linear",
              cost = 0.01, scale = FALSE)
plot(svmfit3, taricJg.df, GameTime ~ Vision)

taric.train = sample (1: nrow (taricJg.df), 150) #this partitions taricJg.df's data into a TRAINING set of 150 observations
taric.test = taricJg.df[-taric.train , ] #151 test observations

Winpred = predict (bestmod, taric.test)
table (predict = Winpred , truth = taric.test$WinLoss) #accurate 97.35% of the time

Winpred2 = predict (svmfit2, taric.test)
table (predict = Winpred2, truth = taric.test$WinLoss) #99.34% accurate for the test data

Winpred3 = predict (svmfit, taric.test)
table (predict = Winpred3, truth = taric.test$WinLoss) #98.68% accurate for the test data

svmfit4 = svm (WinLoss ~., data = taricJg.df[taric.train, ], kernel = "radial",
                gamma = 1, cost = 1)
plot(svmfit4, taricJg.df, GameTime ~ KillDff)
Winpred4 = predict(svmfit4, taric.test)
table(predict = Winpred4, truth = taric.test$WinLoss) #only 59.6% accurate when using a nonlinear classifier

svmfit5 = svm (WinLoss ~., data = taricJg.df[taric.train, ], kernel = "polynomial",
               gamma = 1, cost = 1)
plot(svmfit5, taricJg.df[taric.train, ], GameTime ~ KillDff)

Winpred5 = predict(svmfit5, taric.test)
table(predict = Winpred5, truth = taric.test$WinLoss) #96.69% accurate for test data
Winpred6 = predict(svmfit5, taricJg.df[taric.train,])
table(predict = Winpred6, truth = taricJg.df[taric.train,]$WinLoss) #100% accurate for the training data. As expected, decreased
#classification accuracy for test data vs training data

scaleddata = scale(taricJg.df)
normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
maxmindf = as.data.frame(lapply(taricJg.df, normalize))
# Training and Test Data
taric.train = maxmindf[1:150, ]
taric.test = maxmindf[151:301, ]

nnet = neuralnet(WinLoss ~., data=taric.train, hidden=c(16, 12, 8, 4), linear.output=FALSE, threshold=0.01)

#most accurate machine learning model was with four hidden layers with 10, 10, 5, and 3 neurons respectively. 98% accurate
#for 101 test observations

?neuralnet
nnet$result.matrix
plot(nnet, show.weights = F)

#Test the resulting output
temp_test = subset(taric.test, select = c("GameTime","ItemsCompleted", "Kills", "Deaths", "Assists", "CSPerMinute", "Vision",
                                           "Conqueror", "Glacial", "PTA", "Sunderer", "Shurelyias", "NoMythic",
                                           "Ghost", "Flash", "Ranged", "Kayn", "TowerAdv", "DragonDff", "Baron",
                                           "KillDff"))
head(temp_test)
nnet.results = neuralnet::compute(nnet, temp_test)
results = data.frame(actual = taric.test$WinLoss, prediction = nnet.results$net.result)

results

roundedresults = sapply(results, round, digits=0)
roundedresultsdf= data.frame(roundedresults)
attach(roundedresultsdf)
table(roundedresultsdf) #the neural network is 99.33% accurate for test data
