library(yarrr) #the library command opens whatever installed package you wish to open and 
#imports commands within that package
library(car) #opens car package for this R file
library(manipulateWidget)
library(shiny)
library(rgl) #loads the rgl package which will allow us to view some 3D scatterplots of taric jg data
library(readxl)
library(ggplot2)
library(tidyr)
library(tree)
library(ipred)
library(rpart)

setwd("/Users/User/Desktop/League of Legends Excel Databases")
getwd()
taricJg_data = read_excel("TaricGames.xlsx")
taricJg.df = taricJg_data
quantJg_data = read_excel("TaricQuant.xlsx")
quantJg.df = quantJg_data

head(taricJg.df)
summary(taricJg.df)
plot(taricJg.df)

plot(quantJg.df)
attach(quantJg.df) #makes it so much easier to plot things

TowerKills.lm = lm(TowerAdv ~ KillDff)
abline(TowerKills.lm) #plots the line of best fit for the linear model
summary(TowerKills.lm)

names(taricJg.df)
par(mfrow=c(2,2))
plot(taricJg.df$Kills ~ taricJg.df$Assists)
plot(taricJg.df$Kills ~ taricJg.df$Deaths)
plot(taricJg.df$Kills ~ taricJg.df$CSPerMinute)
plot(taricJg.df$WinLoss ~ taricJg.df$Vision) #first term gets plotted on the y axis, second term gets plotted on the x

attach(taricJg.df)

#we're going to make a few plots relating game time to several variables

par(mfrow=c(2,2))
plot(WinLoss ~ GameTime, main = "Game Time vs Win Frequency", ylab = "Chance of Winning")
plot(KillDff ~ GameTime, main = "Game Time vs Kill Difference", ylab = "Kill Difference") 
#does not seem to be much of a relationship here
plot(Baron ~ GameTime, main = "Game Time vs Baron Secure", ylab = "Chance of Baron secured")
plot(CSPerMinute ~ GameTime, main = "Game Time vs CS Score", ylab = "CS per Minute") 
#maybe a slight negative relationship exists here

#note that the hatvalues command, used to identify outliers, CANNOT be applied to dataframes!

predictJg_logistic.fits = glm(WinLoss ~ Conqueror + Glacial + PTA + Ghost + Flash + Melee + Ranged + Kayn, 
                              data = taricJg.df, family = binomial)

#creates a predictive logistic model of 6 predictors and 1 response
#aims to predict the odds of winning a given game before it has begun

plot(predictJg_logistic.fits)
summary(predictJg_logistic.fits)

#terrible model as none of our p values are statistically significant with enormous standard errors
#after changing some of the binary terms the model improved slightly but all p values are still 0.5 or greater
#ranged variable is not included for some reason

predictJg_logistic.aic = step(predictJg_logistic.fits)

summary(predictJg_logistic.aic)

#p values are somewhat better with multiple variables eliminated but the model is still very bad
#after changing all of the terms, the AIC eliminates every predictor from the model and intercept is not statistically significant
#third update: aic eliminates all except conqueror, glacial, and PTA and all have p values = 0.985. this is for n = 152
#fourth update: aic eliminates all except conq, glacial, pta, flash, and ghost. all have p values > 0.99. this is for n = 200

predictJg_logistic2 = glm(WinLoss ~ Sunderer + Conqueror, data = taricJg.df)
summary(predictJg_logistic2)
plot(predictJg_logistic2)
vif(predictJg_logistic2) #very low collinearity here with VIF 1.2 for both

predictJg_logistic3 = glm(WinLoss ~ Shurelyias + PTA, data = taricJg.df)
summary(predictJg_logistic3) #not a statistically significant model
vif(predictJg_logistic3) #no observed collinearty

vision_logistic.fits = glm(WinLoss ~ Vision, data = taricJg.df, family = binomial)
plot(vision_logistic.fits)
summary(vision_logistic.fits) #both the intercept and slope coefficients are statistically significant
#retains statistical significance tho with p = 0.01 instead of 0.003 at n = 200
#excellent statistical significance p = 0.0007 at n = 225. 

#POSITIVE association between vision and winloss B1 = 0.03732
#slight positive association overall wr 0.05 = B1 n = 225

names(vision_logistic.fits)

par(mfrow=c(1,1))
plot(vision_logistic.fits$fitted.values ~ vision_logistic.fits$residuals)

vision_logistic.fits$linear.predictors #not sure what this does..
names(vision_logistic.fits)

vision_logistic.fits$residuals #lists all the residuals for every instance of x in the logistic model
names(vision_logistic.fits)

vision_logistic.fits$coefficients #indicates an intercept of -1.431 and a positive slope of 0.0542
confint(vision_logistic.fits) #this proves that the intercept and slope are almost certainly nonzero since 0 is not included
#in the 95% confidence interval. null is rejected

names(taricJg.df)

#NEW LOGISTIC MODEL HERE

par(mfrow=c(2,2))
winrate_logistic = glm(WinLoss ~ Kills + Deaths + Assists + CSPerMinute + Vision + PTA + Shurelyias + Sunderer + Conqueror
                       + NoMythic + Melee + Ranged + Kayn, data = taricJg.df, family = binomial)
plot(winrate_logistic)
summary(winrate_logistic)
#most of our predictors were found to NOT be statistically significant
#increasing sample size to n = 200 has helped the model. most predictors statistically significant or borderline significant.
#worst predictor in this model is sunderer p = 0.879

#n = 225, conq and shurelyias now statistically significant. nomythic and melee lowish p values. sunderer still worst p = 0.86

winrate_logistic.aic = step(winrate_logistic)

summary(winrate_logistic.aic) #most predictors are statistically significant or borderline significant
#statistical significance achieved for shurelyias and conqueror predictors. NoMythic borderline significance

#for n = 200, all predictors found to be significant except kills which was borderline significant p = 0.098

#after removing all negative binary inputs B1 coefficients make much more sense. NEGATIVE association with winning found for:
#Deaths, Vision, Shurelyias, Conqueror, and NoMythic. POSITIVE association found for kills, assists, and CSperMin
#vision and shurelyias coefficients are suspect, rest are straightforward

vif(winrate_logistic.aic)
#VIF is better for n = 200 model indicating smaller collinearities

#fairly good VIF values with none above 4.5
#VIF improved with n = 225, no values above 4.1

vif(winrate_logistic) #here we had high VIF values (probable collinearities) for shurelyais, sunderer, and assists

#lets try another model...

names(taricJg.df)
winrate_logistic3 = glm(WinLoss ~ Kills + Deaths + Assists + CSPerMinute + Vision + PTA + Shurelyias + Sunderer + Conqueror
                        + Glacial + NoMythic + Ghost + Flash + Melee + Ranged + Kayn, data = taricJg.df, family = binomial)
plot(winrate_logistic3)
summary(winrate_logistic3)

#most of the qualitative predictors not found to be statistically significant excepting shurelyias and nomythic
#melee has low p value of 0.11

#nomythic no longer significant at n = 225 but shurelyias still significant. melee p val = 0.25

winrate_logistic3.aic = step(winrate_logistic3) #this leaves us with the exact same model we had with winrate_logistic.aic above

#for n = 200 the model is different and keeps many of the qualitative predictors: nomythic, conq, shurelyias, melee, and ghost
summary(winrate_logistic3.aic)
names(winrate_logistic3.aic)
winrate_logistic3.aic$coefficients #this gives us the coefficients for the linear model

#this is a surprisingly reasonable model. all coefficients statistically significant except ghost, mlee, and intercept
#though all 3 have p values 0.166 or below. Kills is borderline significant

#many of the values for coefficients do not make sense so this will need a lot of work...
#after removing all negative values, coefficients are better but there are still many issues (large negative B1 for shurelyias,
#melee, and small neg for vision)

SundererWins_logistic = glm(WinLoss ~ Sunderer, data = taricJg.df, family = binomial)
summary(SundererWins_logistic)
confint(SundererWins_logistic) #minimal to no relationship exists between buying Sunderer and Winning or Losing
#p value = 0.163. relationship could be established given changes in other binomial values
#p value is unchanged from before
#p value dropped to 0.0691 for n = 200
#p value dropped to 0.0332 for n = 225. predictor IS statistically significant

#sunderer shown to have a POSITIVE association with winrate B1 = 0.34
#this value changed to 0.362 for n = 225

#no relationship between running PTA and winning. p value = 0.869
#p value dropped slightly to 0.802 after change
#p value dropped to 0.379 for n = 200
#p value increased to 0.478 for n = 225 still not significant
#p value decreased to 0.237 for n = 300

#assuming B1 != 0 the slope is 0.2776 (issue is large standard dev for coefficient)

ConqWins_logistic = glm(WinLoss ~ Conqueror, data = taricJg.df, family = binomial)
summary(ConqWins_logistic) #no relationship between running conqueror and winning. p value = 0.928
#p value dropped slightly to 0.876 after change
#p value dropped again to 0.738 after fixing an error in the data
#p value dropped to 0.311 for n = 200
#p value dropped to 0.302 for n = 225 still not significant
#p = 0.394 for n = 300

ConqWins_logistic = glm(WinLoss ~ Conqueror*Sunderer, data = taricJg.df, family = binomial)
summary(ConqWins_logistic)

#seems to reject evidence in favor of an interaction between conqueror and sunderer

ShurelyiasWins_logistic = glm(WinLoss ~ Shurelyias, data = taricJg.df, family = binomial)
summary(ShurelyiasWins_logistic) #no relationship between running Shurelyias and winning (slightly better than Conqueror)
#p value = 0.900
#shurelyais data was not changed and neither did the p value
#improved p value of 0.746 for n = 200. Still not statistically significant
#improved to p = 0.705 for n = 225
#improved to p = 0.319 for n = 300

#assuming B1 != 0 there is negative association -0.0966 between shurelyias and winning

GlacialWins_logistic = glm(WinLoss ~ Glacial, data = taricJg.df, family = binomial)
summary(GlacialWins_logistic) #better relationship but still not statistically significant to establish anything meaningful
#p value = 0.593
#slight improvement. new p value = 0.506 for n = 200
#worse p value p = 0.648 for n = 225
#p = 0.291 for n = 300

#assuming B1 != 0, slope is -0.2642

NoMythic_logistic = glm(WinLoss ~ NoMythic, data = taricJg.df, family = binomial)
summary(NoMythic_logistic) #intercept is not statistically significant but slope p value is 0.0848 which is almost 0.05
#good chance there is a meaningful relationship here
#altering data did not change the p value
#borderline statistical significance p value = 0.0564 for n = 200
#this predictor IS statistically significant p = 0.0246

#strong positive association between nomythic and winning B1 = 0.4644
#fixed to strong negative association for same value after changing sign

GhostWins_logistic = glm(WinLoss ~ Ghost, data = taricJg.df, family = binomial)
summary(GhostWins_logistic) #potential for a relationship regarding the slope if other binary values are altered. p value is 0.490
#p value = 0.442 for n = 200
#p value = 0.474 for n = 225
#p value = 0.310 for n = 300

FlashWins_logistic = glm(WinLoss ~ Flash, data = taricJg.df, family = binomial)
summary(FlashWins_logistic) #no statistical significance. p value = 0.616 for flash
#worse p value for n = 200. p = 0.663
#p val = 0.688 for n = 225
#p val = 0.394 for n = 300

MeleeWins_logistic = glm(WinLoss ~ Melee, data = taricJg.df, family = binomial)
summary(MeleeWins_logistic) #no relationship. p value = 0.987
#p value unchanged for n = 200
#p value 0.251 for n = 300

RangedWins_logistic = glm(WinLoss ~ Ranged, data = taricJg.df, family = binomial)
summary(RangedWins_logistic) #also no relationship. p value = 0.987
#improved fit for n = 200. p = 0.734
#p val = 0.474. slight potential for statistical significance with additional observations
#p val = 0.2508 for n = 300

KaynWins_logistic = glm(WinLoss ~ Kayn, data = taricJg.df, family = binomial)
summary(KaynWins_logistic)
#pval = 0.395

par(mfrow=c(1,1))
plot(taricJg.df$CSPerMinute)

CSWins_logistic = glm(WinLoss ~ CSPerMinute, data = taricJg.df, family = binomial)
summary(CSWins_logistic) #very strong statistical significance between CS per min and winning/losing. pval far below 0.05

names(taricJg.df) #returns list of variables so we can make logistic models for the remaining variables

TimeWins_logistic = glm(WinLoss ~ GameTime, data = taricJg.df, family = binomial)
summary(TimeWins_logistic) #is there a relationship between game time and winning or losing?

#pval = 0.229 with z score of 1.2 deviations from null (0). Likely no relationship between game time and winning/losing

BaronWins_logistic = glm(WinLoss ~ Baron, data = taricJg.df, family = binomial)
summary(BaronWins_logistic) #strong positive association between securing Baron and winning p val far below 0.05

TowerWins_logistic = glm(WinLoss ~ TowerAdv, data = taricJg.df, family = binomial)
summary(TowerWins_logistic) #also very strong relationship between toer advantage and winning p val far below 0.05

KillWins_logistic = glm(WinLoss ~ KillDff, data = taricJg.df, family = binomial)
summary(KillWins_logistic) #slightly less powerful relationship between kill difference and winning/losing but still very 
#significant

DragonWins_logistic = glm(WinLoss ~ DragonDff, data = taricJg.df, family = binomial)
summary(DragonWins_logistic) #very strong relationship between dragon advantage and winning/losing p value comparable to Baron

par(mfrow=c(3,3))

newdata3 <- data.frame(CSPerMinute=seq(min(taricJg.df$CSPerMinute), max(taricJg.df$CSPerMinute),len=500))
newdata3$WinLoss = predict(CSWins_logistic, newdata3, type="response")
plot(WinLoss ~ CSPerMinute, data=taricJg.df, col="steelblue", main = "CS model. 68% accurate")  
#THIS IS FOR PLOTTING THE LOGISTIC MODEL
lines(WinLoss ~ CSPerMinute, newdata3, lwd=2)

CSWins.prob = predict(CSWins_logistic, type="response")
CSWins.pred = rep("0", dim(taricJg.df)[1])
CSWins.pred[CSWins.prob > .5] = "Win"        #THIS IS FOR MAKING A TABLE TO ASSESS ITS ACCURACY
CSWins.pred[CSWins.prob < .5] = "Loss"
table(CSWins.pred, taricJg.df$WinLoss)

#this very simple logisitic model correctly predicts game result 68% of the time

newdata4 <- data.frame(Baron=seq(min(taricJg.df$Baron), max(taricJg.df$Baron),len=500))
newdata4$WinLoss = predict(BaronWins_logistic, newdata4, type="response")
plot(WinLoss ~ Baron, data=taricJg.df, col="red", main = "Baron model. 75% accurate")  #THIS IS FOR PLOTTING THE LOGISTIC MODEL
lines(WinLoss ~ Baron, newdata4, lwd=2)

BaronWins.prob = predict(BaronWins_logistic, type="response")
BaronWins.pred = rep("0", dim(taricJg.df)[1])
BaronWins.pred[BaronWins.prob > .5] = "Win"        #THIS IS FOR MAKING A TABLE TO ASSESS ITS ACCURACY
BaronWins.pred[BaronWins.prob < .5] = "Loss"       #we use the Bayes Classifier to make the division here (> 0.5) = win
table(BaronWins.pred, taricJg.df$WinLoss)

#Baron secures correctly predict wins 75% of the time

newdata5 <- data.frame(DragonDff=seq(min(taricJg.df$DragonDff), max(taricJg.df$DragonDff),len=500))
newdata5$WinLoss = predict(DragonWins_logistic, newdata5, type="response")
plot(WinLoss ~ DragonDff, data=taricJg.df, col="orange", main = "Dragon model. 74% accurate")  
#THIS IS FOR PLOTTING THE LOGISTIC MODEL
lines(WinLoss ~ DragonDff, newdata5, lwd=2)

DragonWins.prob = predict(DragonWins_logistic, type="response")
DragonWins.pred = rep("0", dim(taricJg.df)[1])
DragonWins.pred[DragonWins.prob > .5] = "Win"        #THIS IS FOR MAKING A TABLE TO ASSESS ITS ACCURACY
DragonWins.pred[DragonWins.prob < .5] = "Loss"       #we use the Bayes Classifier to make the division here (> 0.5) = win
table(DragonWins.pred, taricJg.df$WinLoss)

#Dragon difference correctly predicts wins 74% of the time (very slightly worse than Baron secures)

newdata6 = data.frame(KillDff = seq(min(taricJg.df$KillDff), max(taricJg.df$KillDff), len = 500))
newdata6$WinLoss = predict(KillWins_logistic, newdata6, type = "response")
plot(WinLoss ~ KillDff, data = taricJg.df, col = "brown", main = "Kill model. 96% accurate")
lines(WinLoss ~ KillDff, newdata6, lwd = 2)

KillWins.prob = predict(KillWins_logistic, type="response")
KillWins.pred = rep("0", dim(taricJg.df)[1])
KillWins.pred[KillWins.prob > .5] = "Win"        #THIS IS FOR MAKING A TABLE TO ASSESS ITS ACCURACY
KillWins.pred[KillWins.prob < .5] = "Loss"       #we use the Bayes Classifier to make the division here (> 0.5) = win
table(KillWins.pred, taricJg.df$WinLoss)

#Kill difference correctly predicts wins 96% of the time. Excluding one observation which was due to a game bug causing no one to
#win, this is 96.33% accurate. Will need to determine if Kills has covariance with other factors

newdata7 = data.frame(GameTime = seq(min(taricJg.df$GameTime), max(taricJg.df$GameTime), len = 500))
newdata7$WinLoss = predict(TimeWins_logistic, newdata7, type = "response")
plot(WinLoss ~ GameTime, data = taricJg.df, col = "green", main = "Time model. 55% accurate")
lines(WinLoss ~ GameTime, newdata7, lwd = 2)

TimeWins.prob = predict(TimeWins_logistic, type="response")
TimeWins.pred = rep("0", dim(taricJg.df)[1])
TimeWins.pred[TimeWins.prob > .5] = "Win"        
TimeWins.pred[TimeWins.prob < .5] = "Loss"       
table(TimeWins.pred, taricJg.df$WinLoss)

#not a very good model. Huge errors. Frequently misclassifying losses as wins. 55% accurate overall, or slightly better than a
#coinflip. Odds of winning gradually increase with time

PTAWins_logistic = glm(WinLoss ~ PTA, family = binomial)

newdata8 = data.frame(PTA = seq(min(taricJg.df$PTA), max(taricJg.df$PTA), len = 500))
newdata8$WinLoss = predict(PTAwins_logistic, newdata8, type = "response")
plot(WinLoss ~ PTA, data = taricJg.df, col = "gold", main = "PTA model. ")
lines(WinLoss ~ PTA, newdata8, lwd = 2)

PTAWins.prob = predict(PTAWins_logistic, type = "response")
PTAWins.pred = rep("0", dim(taricJg.df)[1])
PTAWins.pred[PTAWins.prob > 0.5] = "Win"        
PTAWins.pred[PTAWins.prob < 0.5] = "Loss"      
table(PTAWins.pred, taricJg.df$WinLoss) #not sure why this table is broken

#also a poor model as PTA does not have a strong association with winning.

names(taricJg.df)

newdata9 = data.frame(TowerAdv = seq(min(taricJg.df$TowerAdv), max(taricJg.df$TowerAdv), len = 500))
newdata9$WinLoss = predict(TowerWins_logistic, newdata9, type = "response")
plot(WinLoss ~ TowerAdv, data = taricJg.df, col = "gray", main = "Tower model. 97% accurate")
lines(WinLoss ~ TowerAdv, newdata9, lwd = 2)

TowerWins.prob = predict(TowerWins_logistic, type = "response")
TowerWins.pred = rep("0", dim(taricJg.df)[1])
TowerWins.pred[TowerWins.prob > 0.5] = "Win"
TowerWins.pred[TowerWins.prob < 0.5] = "Loss"
table(TowerWins.pred, taricJg.df$WinLoss)

#excellent model. Tower difference is accurate approximately 97% of the time. There is a potential outlier in the plot with a
#data point indicating a loss despite a tower advantage of 11. Will require investigation

ItemWins_logistic = glm(WinLoss ~ ItemsCompleted, family = binomial)
summary(ItemWins_logistic) #ItemsCompleted is a statistically significant predictor

newdata10 = data.frame(ItemsCompleted = seq(min(taricJg.df$ItemsCompleted), max(taricJg.df$ItemsCompleted), len = 500))
newdata10$WinLoss = predict(ItemWins_logistic, newdata10, type = "response")
plot(WinLoss ~ ItemsCompleted, data = taricJg.df, col = "purple", main = "Item model. 63% accurate")
lines(WinLoss ~ ItemsCompleted, newdata10, lwd = 2)

ItemWins.prob = predict(ItemWins_logistic, type = "response")
ItemWins.pred = rep("0", dim(taricJg.df)[1])
ItemWins.pred[ItemWins.prob > 0.5] = "Win"
ItemWins.pred[ItemWins.prob < 0.5] = "Loss"
table(ItemWins.pred, taricJg.df$WinLoss)

#mediocre model. correctly predicted win/loss 63% of the time. Did an excellent job predicting wins but was only 31.9% accurate
#when predicting losses

names(taricJg.df)

VisionWins_logistic = glm(WinLoss ~ Vision, family = binomial)

newdata11 = data.frame(Vision = seq(min(taricJg.df$Vision), max(taricJg.df$Vision), len = 500))
newdata11$WinLoss = predict(VisionWins_logistic, newdata11, type = "response")
plot(WinLoss ~ Vision, data = taricJg.df, col = "light blue", main = "Vision model. 61% accurate")
lines(WinLoss ~ Vision, newdata11, lwd = 2)

VisionWins.prob = predict(VisionWins_logistic, type = "response")
VisionWins.pred = rep("0", dim(taricJg.df)[1])
VisionWins.pred[VisionWins.prob > 0.5] = "Win"
VisionWins.pred[VisionWins.prob < 0.5] = "Loss"
table(VisionWins.pred, taricJg.df$WinLoss)

#also a mediocre model. 61% accurate so it is slightly worse than the Item model. Unlike the item model it isn't skewed in
#predicting wins or losses but is not very good at either.

winrateAIC.prob = predict(winrate_logistic.aic, type="response")
winrateAIC.pred = rep("0", dim(taricJg.df)[1])
winrateAIC.pred[winrateAIC.prob > .5] = "Win"
winrateAIC.pred[winrateAIC.prob < .5] = "Loss"
table(winrateAIC.pred, taricJg.df$WinLoss)

#model is 91% accurate over all training data vs 89% accurate for our glacial games model

#we want to see how good it is when applied to test data. We have a few methods for this, beginning with validation. Then cross
#validation for k folds. Goal is the smallest drop in accuracy possible for the training values compared to test values

set.seed (1)
train = sample (300, 150)

winrate_logistic = glm(WinLoss ~ Kills + Deaths + Assists + CSPerMinute + Shurelyias + Melee + Kayn, data = taricJg.df,
                       subset = train) #renaming winrate_logistic.aic model to winrate_logistic
mean((WinLoss - predict(winrate_logistic, taricJg.df))[-train ]^2)

#the MSE for the TEST data is 0.09636. MSE should not be as large given that this is a logistic model 
#with outputs between 1 and 0. Nevertheless, 0.09636 is quite good

train2 = sample(300, 100) #new training set of 100 observations
winrate_logistic = glm(WinLoss ~ Kills + Deaths + Assists + CSPerMinute + Shurelyias + Melee + Kayn, data = taricJg.df,
                       subset = train2)
mean((WinLoss - predict(winrate_logistic, taricJg.df))[-train2 ]^2)

#here the MSE for the test data is 0.10151. This is an INCREASE in average error for 200 test observations as opposed to 150.
#this is likely due to having fewer training observations (100 vs 150) leading to increased variation

train3 = sample(300, 50) #new training set of 50 observations. like before, we expect a higher MSE here
winrate_logistic = glm(WinLoss ~ Kills + Deaths + Assists + CSPerMinute + Shurelyias + Melee + Kayn, data = taricJg.df,
                       subset = train3)
mean((WinLoss - predict(winrate_logistic, taricJg.df))[-train3 ]^2)

#the average test error is 0.1209 indicating a worsened fit, as expected

#lets make a new model incorporating all 21 predictors, preform the AIC elimination method, and then apply test methods to it.
#we will call this model winrate_logistic4

winrate_logistic4 = glm(WinLoss ~ GameTime + ItemsCompleted + Kills + Deaths + Assists + CSPerMinute + Vision + Conqueror
                        + Glacial + PTA + Sunderer + Shurelyias + NoMythic + Ghost + Flash + Melee + Kayn + TowerAdv + DragonDff
                        + Baron + KillDff, family = "binomial")
par(mfrow=c(2,2))
plot(winrate_logistic4)
summary(winrate_logistic4) #nothing is statistically significant. This is a huge problem

winrate_logistic4.aic = step(winrate_logistic4)
summary(winrate_logistic4.aic) #the AIC does not fix this issue. There are several possible problems. Lets begin with a simpler
#model only factoring in statistically significant predictors

names(taricJg.df)

winrate_logistic5 = glm(WinLoss ~ ItemsCompleted + Vision + Sunderer + Kills + Deaths + Assists + CSPerMinute + NoMythic +
                          TowerAdv + DragonDff + Baron + KillDff, family = "binomial")
plot(winrate_logistic5) #this looks considerably better
summary(winrate_logistic5) #many of our variables are not statistically significant with TowerAdv, Baron, and KillDff all being
#statistically significant. we can address this with the AIC

winrate_logistic5.aic = step(winrate_logistic5)
#AIC eliminates all variables except TowerAdv, Baron, and KillDff

plot(winrate_logistic5.aic)
summary(winrate_logistic5.aic) #all predictors achieve strong statistical significance

winrate_logistic6 = glm(WinLoss ~ GameTime + ItemsCompleted + Kills + Deaths + Assists + CSPerMinute + Vision + Conqueror
                        + Glacial + PTA + Sunderer + Shurelyias + NoMythic + Ghost + Flash + Melee + Kayn + TowerAdv + DragonDff
                        , family = binomial)
plot(winrate_logistic6)
summary(winrate_logistic6)
winrate_logistic6.aic = step(winrate_logistic6)
summary(winrate_logistic6.aic)

library(boot)

#what we are going to do here is make a regression TREE between WinLoss and every predictor variable in TaricData excluding Ranged

WinPred = factor(ifelse (WinLoss < 0.5, "Loss", "Win") )
tree.taric = tree(WinPred ~ Deaths + Assists + Conqueror + Glacial + NoMythic + Flash + TowerAdv + DragonDff, taricJg.df)
tree.taric = tree(WinLoss ~ Kills + Deaths + Assists + CSPerMinute + Shurelyias + Melee + Kayn -Ranged, taricJg.df)

summary(tree.taric)
par(mfrow=c(1,1))
plot(tree.taric)
names(tree.taric)

text(tree.taric, pretty = 0) #this labels the tree

#our training error rate is about 9.7%. That means the tree is making the right calls about 90.3% of the time. That's pretty good
#we can now use VALIDATION to determine the important metric, the TEST ERROR rate

set.seed (2)
train = sample (1: nrow (taricJg.df), 150) #this partitions taricJg.df's data into a TRAINING set of 150 observations
taric.test = taricJg.df[-train , ] #and a TEST set of equal amount. This even split should avoid some issues with variability
WinPred.test = WinPred[-train] #This code sets WinPred.test to the prediction of win or loss based on the TEST data
tree.taric = tree (WinPred ~ Kills + Deaths + Assists + CSPerMinute + Shurelyias + Melee + Kayn -Ranged, taricJg.df,
                         subset = train) 
taric.pred = predict(tree.taric, taric.test, type = "class") #this compares the trees prediction to the test predictions
table(taric.pred, WinPred.test) #the tree did fairly good. Test error rate is 16.67% for VALIDATION method

set.seed (7)
cv.taric = cv.tree (tree.taric, FUN = prune.misclass) #here we are using CROSS VALIDATION to prune our tree. The purpose of this 
#is to find the ideal number of external nodes to obtain the greatest test error accuracy
names (cv.taric)

cv.taric$size #nodes range from 1 thru 6 and 10
cv.taric$k #this is the pruning parameter. It adds an error penalty the more nodes there are
cv.taric$dev #this is the number of cross validation errors
#it seems the trees with 10 and 6 nodes both result in 22 cv errors. The less errors we have, the better the model!

par (mfrow = c(2, 3))
plot (cv.taric$size , cv.taric$dev, type = "b")
plot (cv.taric$k, cv.taric$dev, type = "b")

prune.taric = prune.misclass (tree.taric, best = 6)
plot(prune.taric)
text(prune.taric, pretty = 0) #this plots our pruned tree with only 6 nodes

tree.pred = predict(prune.taric, taric.test, type = "class") #compares the predicted values of the pruned tree to the actual
#test values of wins and losses stored in taric.test
table(tree.pred, WinPred.test) #cross validation did not result in any improvement to our model. Test error rate is still 16.67%
#error rate for losses is 29.9%, error rate for wins is 2.8%. Thus the model is 10x more likely to incorrectly identify a loss
#than a win

#decreasing the number of nodes to 2 from 6 increases the overall error rate to 30% confirming the information we
#got from the cv error term - that below 6 nodes leads to increased error and 6 or 10 obtain increased accuracy

names(taricJg.df)

#we can create more trees but including every variable instead of a subset of them as we did above

tree2.taric = tree(WinPred ~ Kills + Deaths + Assists + CSPerMinute + Vision + Conqueror + Glacial + PTA + Sunderer
                  + Shurelyias + NoMythic + Ghost + Flash + Melee + Kayn - Ranged, taricJg.df)
tree2.taric = tree(WinLoss ~ Kills + Deaths + Assists + CSPerMinute + Vision + Conqueror + Glacial + PTA + Sunderer
                   + Shurelyias + NoMythic + Ghost + Flash + Melee + Kayn - Ranged, taricJg.df)
summary(tree2.taric)
par(mfrow = c(1,1))
plot(tree2.taric)
text(tree2.taric, pretty = 0)

taric2.pred = predict(tree2.taric, taric.test, type = "class")
table(taric2.pred, WinPred.test) #this tree is a whopping 93%! accurate for the test data obtained thru VALIDATION method

library (randomForest)

set.seed (1)
names(taricJg.df)
bag.taric = randomForest (WinPred ~. -Ranged, taricJg.df, subset = train, mtry = 15,
                           importance = TRUE) #include 15 predictors and exclude ranged

#WinPred is a FACTOR. randomForest function will assume classifiction forest which is what we want since our response, wins or
#losses, is QUALITATIVE
bag.taric

#the prediction seems to be 100% accurate for the training data

yhat.bag <- predict(bag.taric, newdata = taricJg.df[-train , ])
table(yhat.bag, WinPred.test)

#the prediction is also 100% accurate for the test data

set.seed (1)
taric.rf = randomForest(WinPred ~., taricJg.df, subset = train, mtry = 7, importance = TRUE)
yhat.rf <- predict (rf.taric, newdata = taricJg.df[-train , ])
table(yhat.rf, WinPred.test)

plot(taric.rf)

importance (rf.taric)
varImpPlot (rf.taric)

taric.bag <- bagging(
  formula = WinLoss ~.,
  data = taricJg.df,
  nbagg = 500,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)
taric.bag

#changing the number of trees will change our OOB (out of bag) estimated MSE. For 50 trees its 0.3323,
#for 100 trees it is 0.3327, for 500 trees its 0.3256, and for 1000 trees its 0.326
#also for 10 trees it is 0.3652. So generally, increasing the number of trees will decrease the MSE which means our predictions
#will become more accurate!

oob.err<-double(15)
test.err<-double(15)

for(mtry in 1:15) 
{
  rf=randomForest(WinLoss ~. , taricJg.df , subset = train,mtry=mtry,ntree=400) 
  oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
  
  pred<-predict(rf,taricJg.df[-train,]) #Predictions on Test Set for each Tree
  test.err[mtry]= with(taricJg.df[-train,], mean((WinLoss - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ")
  
}
test.err
oob.err

matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",
        xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))

which.min(oob.err) #out of bag minimum error is at 7 predictors per split

#that is it regarding bagging and now we consider another extension of this: boosting

install.packages("gbm")
library(gbm)

set.seed (1)
taric.boost = gbm(WinLoss ~., data = taricJg.df[train, ],
                       distribution = "bernoulli", n.trees = 5000,
                       interaction.depth = 4)
summary(taric.boost)
plot(taric.boost , i = "Assists")
plot(taric.boost, i = "Deaths")

yhat.boost <- predict (taric.boost ,
                       newdata = taricJg.df[-train , ], n.trees = 5000)
mean((yhat.boost - WinLoss)^2)
